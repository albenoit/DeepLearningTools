''' a test file to check the input data pipeline for time series data.
It simply loads and configures a pipeline to read some demo csv files and plots them along iterations.

TODO: this scripts should be copied to the root project folder and simply run on a working machine with Tensorflow and standard Python3 data libraries (matplotlib at last)
'''
import tensorflow as tf


hparams={
         'nbEpoch':1,
         'tsLengthIn':128,   
         'tsLengthOut':10,   
         'batchSize':1
         }

nbEpoch=hparams['nbEpoch']
na_value_string='N/A' #the textual value that specifies NaN or missing values within the read CSV files

#set here paths to your data used for train, val
raw_data_dir_train = "datasamples/timeseries"
raw_data_filename_extension='*.csv'
temporal_series_length=hparams['tsLengthIn']+hparams['tsLengthOut']
ts_windowing_shift=temporal_series_length//4
nb_train_samples=ts_windowing_shift*2000/temporal_series_length#demo contains 2000 samples
batch_size=hparams['batchSize']
steps_per_epoch=nb_train_samples//batch_size
reference_labels=['startDate', 'stopDate'] #to be used if many labels are generated by the get_input_pipeline_train_val function
labels_cols_nb=2

csv_field_delim=','
record_defaults=[['timestamp'], ['timestamp'], [0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0]]
reference_labels=['startDate', 'stopDate', 'prevDayFreeDay', 'todayFreeDay', 'nextDayFreeDay'] #to be used if many labels are generated by the get_input_pipeline_train_val function


mean_vals = [1043.7930703279815, 937.5039896396833, 587.9572398695195, 42.70257611833159, 46.427388557764374, 41.9650561862, 8.372859834289358, 63.985666757595325, 23.704879281756785, 22.60299797330886, 24.366915626761347]
std_vals = [753.5115459652183, 625.5025770874278, 271.7532042774016, 7.292516897584224, 6.326844458353698, 6.974501426484209, 2.9139284071542098, 18.680419651268164, 2.292511632975457, 2.0780787914960706, 1.516894462249191]


import DataProvider_input_pipeline #only import here to reduce dependencies in serving mode
#load all csv files to use for training
raw_data_files=DataProvider_input_pipeline.extractFilenames(root_dir=raw_data_dir_train, file_extension=raw_data_filename_extension)
#sort files in numeric order wrt the last integer before file extension
raw_data_files=sorted(raw_data_files, key=lambda e: int((e.split('.')[0]).split('_')[-1]))
print('Input files found (SORTED IN TIME)='+str(raw_data_files))

def per_sample_process_function(single_period_data_block_raw, timestamps):
    ''' this custom function is intended to post process each sample independantly.
    Here, it separates it does multiple stuff:
    1) it separates input data and expected outcome (labels)
    2) normalizes the data with respect to the precomputed feature means and deviations measured on the train dataset
    3) it transforms oversampled metadata into simple scalars 
    '''
    print('### per_sample_process_function START')
    print('single_period_data_block', single_period_data_block_raw)
    single_period_data_block_raw=tf.transpose(single_period_data_block_raw)
    timestamps=tf.transpose(timestamps)
    freedays=single_period_data_block_raw[:,-3:]
    print('Free days=',freedays)
    print('single_period_data_block_raw', single_period_data_block_raw[:,:-3])

    yesterday_isfree = freedays[0,0]
    today_isfree = freedays[0,1]
    tomorrow_isfree = freedays[0,2]

    #normalizing data wrt mean and std measured on the train set
    means=tf.constant(mean_vals, shape=[1,len(mean_vals)])
    vars=tf.constant(std_vals, shape=[1,len(mean_vals)])
    ts_all=tf.divide(tf.math.subtract(single_period_data_block_raw[:,:-3],means),vars+1e-6)
    print('ts_all', ts_all)
    print('timestamps',timestamps)
    #stacking labels
    stack = [timestamps[0,0],
                timestamps[hparams['tsLengthIn']-1,-1],
                tf.as_string(yesterday_isfree),
                tf.as_string(today_isfree),
                tf.as_string(tomorrow_isfree)]

    stack=(ts_all[:hparams['tsLengthIn'],:],
            yesterday_isfree,
            today_isfree,
            tomorrow_isfree
            ), ts_all[hparams['tsLengthIn']:,:] #try here to predict all features future 
    print("sample content", stack)
    print('### per_sample_process_function END')
    return stack
        #(single_period_data_block_raw[:-3,:],timestamps_start_stop_freedays)
dataset = DataProvider_input_pipeline.FileListProcessor_csv_time_series(files=raw_data_files,
                                            csv_field_delim=csv_field_delim,
                                            record_defaults_values=record_defaults,
                                            batch_size=batch_size,
                                            epochs=nbEpoch,
                                            temporal_series_length=temporal_series_length,#i.e.hparams['tsLengthIn']+hparams['tsLengthOut']
                                            windowing_shift=ts_windowing_shift,
                                            na_value_string=na_value_string,
                                            labels_cols_nb=labels_cols_nb,
                                            per_sample_preprocess_fn=per_sample_process_function,
                                            selected_cols=None,
                                            shuffle=False)#, postprocess_fn=process_function)

print('dataset', dataset)

import matplotlib.pyplot as plt

for step, batch in enumerate(dataset):
    print("New batch, step=",step)
    #print(batch)
    #print(len(batch))
    ts=batch[0][0].numpy()[0]
    worked_yesterday=batch[0][1].numpy()
    worked_today=batch[0][2].numpy()
    worked_tomorrow=batch[0][3].numpy()
    #print((worked_yesterday, worked_today, worked_tomorrow))
    plt.cla()
    plt.plot(ts)
    plt.pause(0.1)
print('Pipeline finished with ', step, 'iterations')